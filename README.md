# UncertaintyFusion-Diffusion
Uncertainty Estimation Fusion for Optimizing Diffusion Model Inference

This project aims to explore and implement a fusion framework for uncertainty estimation methods in the inference phase of diffusion models. The goal is to improve model stability and generalization while optimizing the sampling strategy without increasing training costs.

## ğŸ” Background

In diffusion generative models, the inference process is typically fixed, lacking an awareness of the uncertainty at key stages of the sampling process. This project combines two mainstream uncertainty estimation techniques:

1. **Last Layer Laplace Approximation (LLLA)**: Used for modeling the posterior distribution of the model output.
2. **Monte Carlo Sampling Variance (MC)**: Multi-sample noise estimation at a critical time step during inference, used to estimate the variance.

We propose a **fusion framework** that dynamically adjusts the sampling path by linearly combining the uncertainty estimates from these two methods.

## ğŸ”§ Project Structure

UncertaintyFusion-Diffusion/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ diffusion_model.py          # Core Diffusion Model
â”œâ”€â”€ uncertainty/
â”‚   â”œâ”€â”€ laplace_estimator.py       # LLLA Implementation
â”‚   â”œâ”€â”€ mc_variance.py             # Monte Carlo Sampling Estimation
â”‚   â””â”€â”€ fusion.py                  # Uncertainty Fusion Strategy
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_llla.py                # Run LLLA independently
â”‚   â”œâ”€â”€ run_mc.py                  # Run MC independently
â”‚   â””â”€â”€ run_fusion.py              # Fusion strategy tuning script
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ analysis.ipynb             # Experimental results analysis and visualization
â”œâ”€â”€ results/
â”‚   â””â”€â”€ fusion_vs_baseline.png     # Sample result charts
â””â”€â”€ README.md

## âœ… Current Progress

- [x] Implemented Last Layer Laplace Approximation module
- [x] Implemented Monte Carlo Sampling module
- [x] Validated both methods independently
- [x] Built fusion framework and conducted initial tests
- [ ] Tuning the fusion strategy (ongoing)
- [ ] Evaluation on generative tasks (planned)

## ğŸ’¡ Next Steps

- Investigating a learnable weight or dynamic control mechanism for fusion weights;
- Scaling the method for larger models like Stable Diffusion;
- Exploring combinations with active sampling or attention mechanisms.

## âœï¸ Author

Graduate student working on uncertainty estimation and inference optimization in generative models.

Feel free to reach out for collaboration or discussion.
